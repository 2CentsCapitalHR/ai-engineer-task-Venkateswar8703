{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48de9f8d",
        "outputId": "ebf9f6ab-9269-47e8-9cb5-7968ddbbe9f3"
      },
      "source": [
        "!pip install python-docx"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.1)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/253.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/253.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "032f0a8d",
        "outputId": "8fff9345-10b6-42ca-ad35-131151d4c3e1"
      },
      "source": [
        "!pip install pypdf"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-5.9.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Downloading pypdf-5.9.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.2/313.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "4o2TPe4OOZtm",
        "outputId": "c89976e1-33e6-4d8e-93fb-532f84b99ead"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://09868767a4c6495e63.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://09868767a4c6495e63.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import os\n",
        "import io\n",
        "import json\n",
        "import time\n",
        "import zipfile\n",
        "import tempfile\n",
        "import shutil\n",
        "import traceback\n",
        "from typing import List, Dict, Any, Tuple\n",
        "\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from docx import Document\n",
        "from docx.enum.text import WD_COLOR_INDEX\n",
        "\n",
        "import gradio as gr\n",
        "from pypdf import PdfReader\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "REFS_DIR = \"adgm_refs\"\n",
        "os.makedirs(REFS_DIR, exist_ok=True)\n",
        "\n",
        "CHECKLIST = [\n",
        "    \"Articles of Association\",\n",
        "    \"Memorandum of Association\",\n",
        "    \"UBO Declaration Form\",\n",
        "    \"Incorporation Application Form\",\n",
        "    \"Register of Members and Directors\",\n",
        "]\n",
        "\n",
        "DOCUMENT_KEYWORDS = {\n",
        "    \"Articles of Association\": [\"articles of association\", \"aoa\"],\n",
        "    \"Memorandum of Association\": [\"memorandum of association\", \"moa\"],\n",
        "    \"UBO Declaration Form\": [\"ubo declaration\", \"ubo\", \"ultimate beneficial owner\"],\n",
        "    \"Incorporation Application Form\": [\"incorporation application\", \"application form\", \"incorporation form\"],\n",
        "    \"Register of Members and Directors\": [\"register of members\", \"register of directors\"],\n",
        "}\n",
        "\n",
        "JURISDICTION_KEYWORDS_WRONG = [\"federal court\", \"federal courts\", \"u.a.e. federal\", \"dubai courts\"]\n",
        "SIGNATURE_KEYWORDS = [\"signature\", \"signed\", \"for and on behalf\", \"authorised signatory\"]\n",
        "AMBIGUOUS_TERMS = [\"may \", \"should \", \"endeavour to\", \"subject to the discretion\", \"best endeavours\"]\n",
        "\n",
        "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
        "\n",
        "def extract_text_from_docx(path: str) -> str:\n",
        "    try:\n",
        "        doc = Document(path)\n",
        "        paragraphs = [p.text for p in doc.paragraphs if p.text and p.text.strip()]\n",
        "        return \"\\n\".join(paragraphs)\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "def extract_text_from_pdf(path: str) -> str:\n",
        "    try:\n",
        "        reader = PdfReader(path)\n",
        "        texts = []\n",
        "        for p in reader.pages:\n",
        "            try:\n",
        "                texts.append(p.extract_text() or \"\")\n",
        "            except Exception:\n",
        "                continue\n",
        "        return \"\\n\".join(texts)\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "def chunk_text(text: str, chunk_size: int = 800, overlap: int = 100) -> List[str]:\n",
        "    if not text:\n",
        "        return []\n",
        "    tokens = text.split()\n",
        "    chunks = []\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "        chunk = tokens[i:i+chunk_size]\n",
        "        chunks.append(\" \".join(chunk))\n",
        "        i += chunk_size - overlap\n",
        "    return chunks\n",
        "\n",
        "class SimpleRAGIndex:\n",
        "    def __init__(self, embed_model_name=EMBED_MODEL_NAME):\n",
        "        self.model = SentenceTransformer(embed_model_name)\n",
        "        self.records = []\n",
        "\n",
        "    def add_text(self, text: str, source: str):\n",
        "        if not text or not text.strip():\n",
        "            return\n",
        "        emb = self.model.encode(text, convert_to_numpy=True)\n",
        "        self.records.append({\"text\": text, \"source\": source, \"emb\": emb})\n",
        "\n",
        "    def build_from_folder(self, folder: str):\n",
        "        self.records = []\n",
        "        for fname in sorted(os.listdir(folder)):\n",
        "            path = os.path.join(folder, fname)\n",
        "            if fname.lower().endswith(\".docx\"):\n",
        "                try:\n",
        "                    if not zipfile.is_zipfile(path):\n",
        "                        print(\"Skipping invalid DOCX reference:\", fname)\n",
        "                        continue\n",
        "                except Exception:\n",
        "                    continue\n",
        "                text = extract_text_from_docx(path)\n",
        "            elif fname.lower().endswith(\".pdf\"):\n",
        "                text = extract_text_from_pdf(path)\n",
        "            else:\n",
        "                continue\n",
        "            for chunk in chunk_text(text, chunk_size=800, overlap=100):\n",
        "                self.add_text(chunk, source=fname)\n",
        "        print(f\"RAG index built with {len(self.records)} chunks.\")\n",
        "\n",
        "    def retrieve(self, query: str, top_k: int = 4) -> List[Dict[str, Any]]:\n",
        "        if not self.records:\n",
        "            return []\n",
        "        q_emb = self.model.encode(query, convert_to_numpy=True)\n",
        "        emb_matrix = np.vstack([r[\"emb\"] for r in self.records])\n",
        "        dots = emb_matrix @ q_emb\n",
        "        qnorm = np.linalg.norm(q_emb) + 1e-12\n",
        "        norms = np.linalg.norm(emb_matrix, axis=1) * qnorm + 1e-12\n",
        "        sims = dots / norms\n",
        "        top_idx = np.argsort(-sims)[:top_k]\n",
        "        results = []\n",
        "        for i in top_idx:\n",
        "            results.append({\"text\": self.records[i][\"text\"], \"source\": self.records[i][\"source\"], \"score\": float(sims[i])})\n",
        "        return results\n",
        "\n",
        "def rag_suggest_clauses(question: str, top_k: int = 3) -> str:\n",
        "    ensure_models()\n",
        "    contexts = RAG_INDEX.retrieve(question, top_k=top_k)\n",
        "    context_text = \"\\n\\n\".join([f\"[source:{c['source']}] {c['text']}\" for c in contexts])\n",
        "\n",
        "    prompt = (\n",
        "        \"You are an ADGM legal compliance assistant. Use the following official ADGM reference contexts to answer accurately.\\n\\n\"\n",
        "        f\"CONTEXTS:\\n{context_text}\\n\\nQUESTION: {question}\\n\\nAnswer concisely:\"\n",
        "    )\n",
        "    try:\n",
        "        response = genai.chat.completions.create(\n",
        "            model=\"models/chat-bison-001\",\n",
        "            messages=[\n",
        "                {\"author\": \"system\", \"content\": \"You are a helpful legal assistant.\"},\n",
        "                {\"author\": \"user\", \"content\": prompt},\n",
        "            ],\n",
        "            temperature=0,\n",
        "            max_tokens=300,\n",
        "        )\n",
        "        return response.candidates[0].content.strip()\n",
        "    except Exception as e:\n",
        "        print(\"Gemini API error:\", e)\n",
        "        return \"Error generating suggestion.\"\n",
        "\n",
        "RAG_INDEX: SimpleRAGIndex = None\n",
        "\n",
        "def ensure_models():\n",
        "    global RAG_INDEX\n",
        "    if RAG_INDEX is None:\n",
        "        RAG_INDEX = SimpleRAGIndex(EMBED_MODEL_NAME)\n",
        "        RAG_INDEX.build_from_folder(REFS_DIR)\n",
        "\n",
        "def classify_document(text: str) -> List[str]:\n",
        "    found = []\n",
        "    ln = (text or \"\").lower()\n",
        "    for name, kws in DOCUMENT_KEYWORDS.items():\n",
        "        if any(k in ln for k in kws):\n",
        "            found.append(name)\n",
        "    return found or [\"Unknown\"]\n",
        "\n",
        "def detect_issues_in_text(text: str) -> List[Dict[str, Any]]:\n",
        "    issues = []\n",
        "    ln = (text or \"\").lower()\n",
        "    for bad in JURISDICTION_KEYWORDS_WRONG:\n",
        "        if bad in ln:\n",
        "            issues.append({\"section\": \"Jurisdiction\", \"issue\": f\"Incorrect jurisdiction reference '{bad}'\", \"severity\": \"High\", \"suggestion\": \"Replace with 'ADGM Courts'.\"})\n",
        "            break\n",
        "    if not any(k in ln for k in SIGNATURE_KEYWORDS):\n",
        "        issues.append({\"section\": \"Execution/Signatory\", \"issue\": \"Missing signature block\", \"severity\": \"High\", \"suggestion\": \"Add signature block with name/title/date.\"})\n",
        "    for term in AMBIGUOUS_TERMS:\n",
        "        if term in ln:\n",
        "            issues.append({\"section\": \"Language\", \"issue\": f\"Ambiguous term '{term.strip()}' used\", \"severity\": \"Medium\", \"suggestion\": \"Use binding/clear language.\"})\n",
        "            break\n",
        "    return issues\n",
        "\n",
        "def insert_comments_and_highlight(input_docx_path: str, issues: List[Dict[str,Any]], out_path: str):\n",
        "    doc = Document(input_docx_path)\n",
        "    for issue in issues:\n",
        "        phrase = issue.get(\"issue\", \"\").split(\":\")[0].strip()\n",
        "        highlighted = False\n",
        "        for p in doc.paragraphs:\n",
        "            if phrase and phrase.lower() in p.text.lower():\n",
        "                run = p.add_run(f\"  [REVIEWER NOTE: {issue['issue']}]\")\n",
        "                try:\n",
        "                    run.font.highlight_color = WD_COLOR_INDEX.YELLOW\n",
        "                except Exception:\n",
        "                    pass\n",
        "                highlighted = True\n",
        "                break\n",
        "        if not highlighted:\n",
        "            p = doc.add_paragraph()\n",
        "            run = p.add_run(f\"[REVIEW] {issue['section']}: {issue['issue']} -- Suggestion: {issue.get('suggestion','')}\")\n",
        "            run.bold = True\n",
        "    doc.save(out_path)\n",
        "\n",
        "def analyze_docx_files(file_paths: List[str]) -> Tuple[Dict[str,Any], str]:\n",
        "    ensure_models()\n",
        "    tmpdir = tempfile.mkdtemp(prefix=\"adgm_review_\")\n",
        "    reviewed_files = {}\n",
        "    detected_types = []\n",
        "    issues_all = []\n",
        "    uploaded_count = 0\n",
        "\n",
        "    for path in file_paths:\n",
        "        if not path.lower().endswith(\".docx\"):\n",
        "            continue\n",
        "        if not zipfile.is_zipfile(path):\n",
        "            print(\"Skipping invalid DOCX:\", path)\n",
        "            continue\n",
        "        fname = os.path.basename(path)\n",
        "        uploaded_count += 1\n",
        "        text = extract_text_from_docx(path)\n",
        "        types = classify_document(text)\n",
        "        detected_types.extend(types)\n",
        "        issues = detect_issues_in_text(text)\n",
        "\n",
        "        for iss in issues:\n",
        "            if iss.get(\"severity\") == \"High\":\n",
        "                try:\n",
        "                    sugg = rag_suggest_clauses(f\"Provide a compliant ADGM clause for: {iss['section']}\", top_k=3)\n",
        "                    iss[\"suggestion\"] = sugg\n",
        "                except Exception:\n",
        "                    iss[\"suggestion\"] = iss.get(\"suggestion\", \"\")\n",
        "\n",
        "        for iss in issues:\n",
        "            issues_all.append({\n",
        "                \"document\": fname,\n",
        "                \"section\": iss.get(\"section\", \"General\"),\n",
        "                \"issue\": iss.get(\"issue\"),\n",
        "                \"severity\": iss.get(\"severity\"),\n",
        "                \"suggestion\": iss.get(\"suggestion\")\n",
        "            })\n",
        "\n",
        "        out_path = os.path.join(tmpdir, f\"reviewed_{fname}\")\n",
        "        try:\n",
        "            insert_comments_and_highlight(path, issues, out_path)\n",
        "        except Exception:\n",
        "            shutil.copy(path, out_path)\n",
        "        reviewed_files[fname] = out_path\n",
        "\n",
        "    process = \"Unknown\"\n",
        "    if any(t in [\"Articles of Association\", \"Memorandum of Association\"] for t in detected_types):\n",
        "        process = \"Company Incorporation\"\n",
        "\n",
        "    required_documents = len(CHECKLIST)\n",
        "    missing_document = None\n",
        "    if process == \"Company Incorporation\":\n",
        "        present = set(detected_types)\n",
        "        missing = [d for d in CHECKLIST if d not in present]\n",
        "        if missing:\n",
        "            missing_document = missing[0]\n",
        "    else:\n",
        "        missing = []\n",
        "\n",
        "    output = {\n",
        "        \"process\": process,\n",
        "        \"documents_uploaded\": uploaded_count,\n",
        "        \"required_documents\": required_documents,\n",
        "        \"missing_document\": missing_document,\n",
        "        \"issues_found\": issues_all\n",
        "    }\n",
        "\n",
        "    zip_path = os.path.join(tmpdir, \"reviewed_docs.zip\")\n",
        "    with zipfile.ZipFile(zip_path, \"w\") as zf:\n",
        "        for orig, p in reviewed_files.items():\n",
        "            if os.path.exists(p):\n",
        "                zf.write(p, arcname=os.path.basename(p))\n",
        "\n",
        "    return output, zip_path\n",
        "\n",
        "def gradio_analyze(input_files):\n",
        "    if not input_files:\n",
        "        return {\"error\": \"No files uploaded\"}, \"\"\n",
        "    paths = []\n",
        "    for f in input_files:\n",
        "        if isinstance(f, str):\n",
        "            paths.append(f)\n",
        "        else:\n",
        "            try:\n",
        "                p = getattr(f, \"name\", None) or f[0]\n",
        "                paths.append(p)\n",
        "            except Exception:\n",
        "                continue\n",
        "    report, zip_path = analyze_docx_files(paths)\n",
        "    return report, zip_path\n",
        "\n",
        "with gr.Blocks(title=\"ADGM Corporate Agent - Docx + Gemini RAG\") as demo:\n",
        "    gr.Markdown(\"# ADGM Corporate Agent — DOCX-only Reviewer with Gemini RAG\")\n",
        "\n",
        "    with gr.Tab(\"Document Analysis\"):\n",
        "        file_input = gr.File(file_count=\"multiple\", file_types=[\".docx\"], label=\"Upload DOCX files\")\n",
        "        analyze_btn = gr.Button(\"Analyze Documents\")\n",
        "        output_json = gr.JSON(label=\"Structured Output\")\n",
        "        download = gr.File(label=\"Download reviewed files (zip)\")\n",
        "\n",
        "        analyze_btn.click(fn=gradio_analyze, inputs=[file_input], outputs=[output_json, download])\n",
        "\n",
        "    with gr.Tab(\"Notes\"):\n",
        "        gr.Markdown(\"This tool accepts only valid `.docx` files. It will skip corrupted/non-docx files.\\n\"\n",
        "                    \"Uses Gemini (Google PaLM API) for RAG-powered suggestions.\\n\"\n",
        "                    \"Place official ADGM reference docs in the `adgm_refs` folder.\")\n",
        "\n",
        "demo.launch(share=True)\n"
      ]
    }
  ]
}